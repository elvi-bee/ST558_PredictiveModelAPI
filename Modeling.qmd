---
title: "Modeling"
author: "Elvira McIntyre"
format: html
editor: visual
---

## 1. Introduction

This project explores data on Diabetes. The original data were obtained from the CDC's Behavioral Risk Factor Surveillance System (BRFSS), a health-related telephone survey that is collected annually by the CDC. The data contain self-reported health and lifestyle information and include 22 variables related to chronic disease, physical activity, general health, demographics, and access to health care. The primary outcome of interest in this project is 'Diabetes_binary', a binary indicator where:

-   0 = No diabetes

-   1 = Diabetes

The majority of remaining variables are coded as binary (0/1) indicators (e.g., HighBP, HighChol, Stroke, Smoker), with some coded as ordinal categories (e.g., Age, Education, Income) or counts (e.g., MentHlth, PhysHlth).

During the Exploratory Data Analysis many variables showed interesting relationships with Diabetes prevalence. Among binary variables, high blood pressure, high cholesterol, heart disease or attack status, physical activity, and difficulty walking all showed relationships with diabetes prevalence. Among numeric variables, BMI and physical health showed the strongest relationships. Among ordinal variables, age and general health appeared to have stronger relations with diabetes that other ordinal variables.
[Click here to go back to the EDA page.](EDA.html)
In this section we will proceed to construct two models using selected variables from the aforementioned data.

### 1.1 Model Definitions

#### 1.1.1 Classification Tree
#### 1.1.2 Random Forest

### 1.2 Variable Selection

Based on the results of the Exploratory Data Analysis, five predictors were selected that showed the strongest relationships with diabetes. **High blood pressure** and **high cholesterol** were two of the most strongly associated binary indicators. **Difficulty walking** was another strong binary predictor. **BMI** was the strongest continuous predictor. Finally, **age** showed a clear relationship, with diabetes prevalence increasing steadily across older age groups.

Summary of variables that were selected:

-   **High Blood Pressure** (high_bp): binary, 0 = no high BP/ 1 = high BP

-   **High Cholesterol** (high_chol): binary, 0 = no high Chol/ 1 = high Chol

-   **BMI** (bmi): continuous, 12 - 98

-   **Age** (age): ordinal, 13-level age categories, 1 = 18-24; 9 = 60-64; 13 = 80 or older

-   **Difficulty Walking** (diff_walk): binary, 0 = no, 1 = yes



## 2. Modeling

Classification Tree Random Forest Models will be constructed and tested to determine the best model for the data. 

### 2.1 Data Preparation
#### 2.1.1 Creating Training and Test Splits and Cross Validation
The data was split into a training (70% of the data) and test set (30% of the data). Then, 5-fold cross-validation is implemented to aid with selection of the ultimate best model.
```{r}
# split training and test data
library(tidyverse)
library(tidymodels)
library(readxl)
library(janitor)
library(rpart.plot)
library(vip)

# Read in data and clean column names
data <- read.csv("data/diabetes_binary_health_indicators_BRFSS2015.csv",header=TRUE) |>
  clean_names()
set.seed(11)

# Factors
data <- data |>
  mutate(diabetes_binary = factor(diabetes_binary))

# 70/30 split
diab_split <- initial_split(data, prop = 0.70, strata = diabetes_binary)
diab_train <- training(diab_split)
diab_test  <- testing(diab_split)

# 5-fold cross-validation
diab_folds <- vfold_cv(diab_train, v = 5, strata = diabetes_binary)

```

#### 2.1.2 Recipe
Define the recipe using the following 5 predictors: high_bp, high_chol, diff_walk, bmi, age. Additionally, this step creates dummy variables for nominal predictors and normalizes numeric predictors. 
```{r}
# Recipe 
diab_rec <- recipe(
  diabetes_binary ~ high_bp + high_chol + diff_walk + bmi + age,
  data = diab_train
) |>
  step_dummy(all_nominal_predictors()) |>
  step_normalize(all_numeric_predictors())
```

### 2.3 Classification Tree
```{r}
# Specify model
tree_spec <- decision_tree(
  tree_depth = tune(),
  cost_complexity = tune(),
  min_n = 20
) |>
  set_engine("rpart") |>
  set_mode("classification")

# Workflow
tree_wf <- workflow() |>
  add_recipe(diab_rec) |>
  add_model(tree_spec)

# Tuning grid and 5-fold cv with log loss, accuracy, roc_auc
tree_grid <- grid_regular(
  cost_complexity(),
  tree_depth(),
  levels = c(5, 3)  
)

tree_res <- tune_grid(
  tree_wf,
  resamples = diab_folds,
  grid = tree_grid,
  metrics = metric_set(mn_log_loss)
)
tree_res |> collect_metrics()

# Select best tree
best_tree <- select_best(tree_res, metric = "mn_log_loss")
best_tree

# Final tree
tree_final <- tree_wf |>
  finalize_workflow(best_tree) |>
  last_fit(diab_split,
           metrics = metric_set(mn_log_loss, accuracy, roc_auc, brier_class)
           )

tree_final |> collect_metrics()


# Confusion matrix for the final tree (test set)
tree_preds <- tree_final |> collect_predictions()

tree_conf <- tree_preds |>
  conf_mat(truth = diabetes_binary, estimate = .pred_class)

tree_conf

# Confusion matrix plot
autoplot(tree_conf)



# Fit final tree on FULL training data for plotting
tree_final_full <- tree_wf |>
  finalize_workflow(best_tree) |>
  fit(data = diab_train)

final_tree_fit <- tree_final_full |>
  extract_fit_engine()

rpart.plot(
  final_tree_fit,
  type = 2,
  extra = 104,
  fallen.leaves = TRUE,
  roundint = FALSE,
  main = "Final Classification Tree for Diabetes"
)
```

### 2.4 Random Forest
```{r}
# Specify model
rf_spec <- rand_forest(
  mtry  = 2,
  trees = 100,
  min_n = 5
) |>
  set_engine("ranger", importance = "impurity") |>
  set_mode("classification")

# Workflow
rf_wf <- workflow() |>
  add_recipe(diab_rec) |>
  add_model(rf_spec)

# 5-fold CV
rf_res <- fit_resamples(
  rf_wf,
  resamples = diab_folds,
  metrics   = metric_set(mn_log_loss)
)

rf_res |> collect_metrics()

# Final RF fit on full training data with evaluation on test set
rf_final <- rf_wf |>
  last_fit(diab_split,
           metrics = metric_set(mn_log_loss, accuracy, roc_auc, brier_class)
           )

rf_final |> collect_metrics()

# Confusion matrix
rf_preds <- rf_final |> collect_predictions()

rf_conf <- rf_preds |>
  conf_mat(truth = diabetes_binary, estimate = .pred_class)

rf_conf

# Confusion matrix plot
autoplot(rf_conf)

# Fit RF on FULL training data for importance
rf_full <- rf_wf |>
  fit(data = diab_train)

rf_engine <- rf_full |>
  extract_fit_engine()

vip(
  rf_engine,
  num_features = 5,
  main = "Variable Importance from Random Forest"
)
```
### 2.5 Comparing Model Results
A comparison of classification Tree and Random Forest Models shows that the Random Forest Model slightly outperforms across all metrics.
Most importantly, the Random Forest model achieved a lower log-loss, at 0.3334360 for Random Forest and 0.3483469 for the Classification Tree.

The random forest also performed better across other metrics, achieving lower test-set Brier score (0.1018 vs. 0.1037), higher ROC AUC (0.795 vs. 0.769), and slightly higher accuracy (0.8636 vs. 0.8631). 

Based on these results the Random Forest Model was selected as the final model to be used in the API component of this project.
```{r}
tree_logloss <- tree_final |>
  collect_metrics() |>
  mutate(model = "Classification Tree") |>
  select(model, .metric, .estimate)

rf_logloss <- rf_final |>
  collect_metrics() |>
  mutate(model = "Random Forest") |>
  select(model, .metric, .estimate)

logloss_comparison <- bind_rows(tree_logloss, rf_logloss)

logloss_comparison
```



```{r}
# Test-set log-loss for each model
tree_test_metrics <- tree_final |>
  collect_metrics() |>
  mutate(model = "Classification Tree")

rf_test_metrics <- rf_final |>
  collect_metrics() |>
  mutate(model = "Random Forest")

model_comparison <- bind_rows(tree_test_metrics, rf_test_metrics)

view(model_comparison)

```


```{r}

```